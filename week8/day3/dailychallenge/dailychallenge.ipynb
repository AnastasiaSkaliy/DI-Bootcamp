{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text corpus from the URL\n",
    "url = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "response = requests.get(url)\n",
    "alice_text = response.text\n",
    "\n",
    "# Preprocess the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([alice_text])\n",
    "\n",
    "# Convert text to sequences and pad sequences for uniform length\n",
    "sequences = tokenizer.texts_to_sequences([alice_text])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# Print a part of the processed sequence for verification\n",
    "print(\"Original Text:\")\n",
    "print(alice_text[:200])\n",
    "\n",
    "print(\"\\nProcessed Sequences:\")\n",
    "print(padded_sequences[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 200 characters of the corpus:\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Con\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(corpus[:\u001b[38;5;241m200\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 5: Tokenize the text using Tokenizer() and create vocabulary\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m()\n\u001b[1;32m     19\u001b[0m corpus \u001b[38;5;241m=\u001b[39m preprocess_text(corpus)\n\u001b[1;32m     20\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts([corpus])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Step 2: Create a preprocess function using regular expressions\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Step 3: Split the text using '*** START' and '*** END', remove irrelevant parts\n",
    "start_idx = alice_text.find('*** START')\n",
    "end_idx = alice_text.find('*** END')\n",
    "corpus = alice_text[start_idx:end_idx]\n",
    "\n",
    "# Step 4: Print the first 200 characters of the corpus\n",
    "print(\"First 200 characters of the corpus:\")\n",
    "print(corpus[:200])\n",
    "\n",
    "# Step 5: Tokenize the text using Tokenizer() and create vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "corpus = preprocess_text(corpus)\n",
    "tokenizer.fit_on_texts([corpus])\n",
    "\n",
    "# Calculate total_words\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"Total words in the vocabulary:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Input And Output Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 6: Create n-gram sequences\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtexts_to_sequences([corpus])[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(word)):\n\u001b[1;32m      5\u001b[0m         n_gram_sequence \u001b[38;5;241m=\u001b[39m word[:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 6: Create n-gram sequences\n",
    "input_sequence = []\n",
    "for word in tokenizer.texts_to_sequences([corpus])[0]:\n",
    "    for i in range(1, len(word)):\n",
    "        n_gram_sequence = word[:i+1]\n",
    "        input_sequence.append(n_gram_sequence)\n",
    "\n",
    "# Step 7: Pad the sequences\n",
    "max_sequence_length = max([len(seq) for seq in input_sequence])\n",
    "padded_sequences = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Print the first few sequences for verification\n",
    "print(\"First few sequences:\")\n",
    "for i in range(5):\n",
    "    print(padded_sequences[i])\n",
    "\n",
    "# Print the shape of the padded array\n",
    "print(\"Shape of padded array:\", padded_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build The Neural Network Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Step 8: Build the Neural Network Model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer for text representation\n",
    "embedding_dim = 100\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "\n",
    "# Add LSTM layers for processing the sequences\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a Dense layer for output prediction\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile And Train The Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Step 9: Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 10: Train the Model with EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Assuming you have labels for the next word in your n-gram sequences\n",
    "labels = padded_sequences[:, -1]\n",
    "input_sequences = padded_sequences[:, :-1]\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_sequences, labels, epochs=50, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('text_generation_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate The Model’s Performance On Test Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, model, tokenizer, max_sequence_length, num_words):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Preprocess the seed text\n",
    "        seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        seed_padded = pad_sequences([seed_sequence], maxlen=max_sequence_length-1, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_word_index = model.predict_classes(seed_padded, verbose=0)\n",
    "        \n",
    "        # Convert the index to the actual word\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        \n",
    "        # Update the seed text for the next iteration\n",
    "        seed_text += \" \" + predicted_word\n",
    "        output_text += \" \" + predicted_word\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Example usage\n",
    "seed_text = \"Alice\"\n",
    "generated_text = generate_text(seed_text, model, tokenizer, max_sequence_length, num_words=50)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
